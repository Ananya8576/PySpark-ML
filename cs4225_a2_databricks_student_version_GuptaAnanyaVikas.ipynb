{"cells":[{"cell_type":"markdown","source":["## Task 1: Spark SQL (15m)"],"metadata":{"id":"yvjBmGBAxnQc","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"36b565b0-dc53-4172-b646-4c82e1c472be","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"],"metadata":{"id":"MkbrHZYEw5Cr","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"30d54257-dc20-4174-aa40-84e1f6abc56f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sales_file_location = \"/FileStore/tables/Sales_table.csv\"\nproducts_file_location = \"/FileStore/tables/Products_table.csv\"\nsellers_file_location = \"/FileStore/tables/Sellers_table.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\nproducts_table = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(products_file_location)\n\nsales_table = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(sales_file_location)\n\nsellers_table = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(sellers_file_location)"],"metadata":{"id":"2luSAeOXxBiQ","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5607f10e-0a58-4330-bbeb-fa1d6863efb1","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# (a) Output the top 3 most popular products sold among all sellers [2m]\n# Your table should have 1 column(s): [product_name]\n\n#Ref: https://www.geeksforgeeks.org/how-to-find-the-sum-of-particular-column-in-pyspark-dataframe/\n\n#join - group product names - total num sold of each - descending - select reqd col - top 3 \nproducts_table.join(sales_table,['product_id']).groupBy(\"product_name\").agg({\"num_of_items_sold\":'sum'}).orderBy(\"sum(num_of_items_sold)\",ascending=False).select(\"product_name\").show(n=3,truncate=False)"],"metadata":{"id":"Ps_v7oTixnQf","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"7fb33021-930c-4fa9-b595-4ed83c279ed4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n|product_name |\n+-------------+\n|product_51270|\n|product_18759|\n|product_59652|\n+-------------+\nonly showing top 3 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# (b) Find out the total sales of the products sold by sellers 1 to 10 and output the top most sold product [2m]\n# Your table should have 1 column(s): [product_name]\n# clarification: Output the top most sold product (in terms of quantity) among sellers with seller_id 1 to 10.\n\n#join - filter for seller_id 1 to 10 - group product names - total num sold of each - descending - select reqd col - top 1\nproducts_table.join(sales_table,['product_id']).filter(sales_table.seller_id.between(1,10)).groupBy(\"product_name\").agg({'num_of_items_sold':'sum'}).orderBy(\"sum(num_of_items_sold)\",ascending=False).select(\"product_name\").show(n=1,truncate=False)"],"metadata":{"id":"Ljmb_1OaxC8Q","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"866983b3-8214-4740-8f4d-90e87d1db482","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n|product_name |\n+-------------+\n|product_36658|\n+-------------+\nonly showing top 1 row\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# (c) Compute the combined revenue earned from sellers where seller_id ranges from 1 to 500 inclusive. [3m]\n# Your table should have 1 column(s): [total_revenue]\n\n#join - filter for seller_id 1 to 500 - get revenue column - add all revenues to get total_revenue\nsales_table.join(products_table,['product_id']).filter(sales_table.seller_id.between(1,500)).withColumn(\"revenue\",sales_table.num_of_items_sold * products_table.price).agg({\"revenue\":\"sum\"}).withColumnRenamed(\"sum(revenue)\", \"total_revenue\").show()"],"metadata":{"id":"QtinRRycxDBS","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fa7bec8e-f93d-48ff-af38-d395c6fe7422","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------+\n|total_revenue|\n+-------------+\n|    160916699|\n+-------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# (d) Among sellers with rating >= 4 who have achieved a combined number of products sold >= 3000, find out the top 10 most expensive product sold by any of the sellers. (If there are multiple products at the same price, please sort them in ascending order of product_id) [8m]\n# Your table should have 1 column(s): [product_name]\n# To get the full mark, your query should not run for more than 1 min\n\n#filter seller rating>=4 - calc total num of items sold by each seller\ncom1=sellers_table.filter(sellers_table.rating>=4).join(sales_table,['seller_id'],\"inner\").groupBy(sales_table.seller_id).agg({\"num_of_items_sold\":\"sum\"}).withColumnRenamed(\"sum(num_of_items_sold)\",\"total_num\")\n\n#filter for num >=3000 - select distinct seller_id\ncom2=com1.filter(com1.total_num>=3000).select(\"seller_id\").distinct()\n\n#join sales & product table - join with the filtered seller_ids - distinct products - order by desc price, asc prod_id - select reqd col - top 10\nsales_table.join(products_table,['product_id'], \"inner\").join(com2,['seller_id'],\"inner\").select(\"price\",\"product_id\",\"product_name\").distinct().orderBy([\"price\",\"product_id\"],ascending=[0,1]).select(\"product_name\").show(n=10,truncate=False)\n"],"metadata":{"id":"jdG80LVMxnQf","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"59c00e0a-34de-4614-b783-71beb7503716","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------+\n|product_name|\n+------------+\n|product_106 |\n|product_117 |\n|product_363 |\n|product_712 |\n|product_843 |\n|product_897 |\n|product_923 |\n|product_1466|\n|product_1507|\n|product_1514|\n+------------+\nonly showing top 10 rows\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["## Task 2: Spark ML (10m)"],"metadata":{"id":"4fziMyvTxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2551ab92-377c-4492-9d99-258610b143a1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate()"],"metadata":{"id":"wtocOKQXxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2ebc093d-9256-4e99-85d3-3d36b50a6053","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["bank_train_location = \"/FileStore/tables/bank_train.csv\"\nbank_test_location = \"/FileStore/tables/bank_test.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\nbank_train = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(bank_train_location)\n\nbank_test = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(bank_test_location)"],"metadata":{"id":"lQB18KhnxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2eee140e-773a-4e76-9f6c-40e809e136b0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Build ML model to predict whether the customer will subscribe bank deposit service or not. Train the model using training set and evaluate the model performance (e.g. accuracy) using testing set. \n* You can explore different methods to pre-process the data and select proper features\n* You can utilize different machine learning models and tune model hyperparameters\n* Present the final testing accuracy."],"metadata":{"id":"YTZevHlAxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"98477bc0-fdf9-4585-8cf2-24b4b0ebc3f1","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# data preparation (4m)\nfrom pyspark.ml.feature import VectorAssembler, StringIndexer, OneHotEncoder\nfrom pyspark.ml.feature import ChiSqSelector\n\ntarget_col = \"label\" #output\ninput_cols = [\"age\", \"job\", \"marital\", \"education\", \"default\", \"balance\", \"housing\", \"loan\", \"contact\", \"day\", \"month\", \"duration\", \"campaign\", \"pdays\", \"previous\", \"poutcome\"] #all inputs \n\n#mainly continuous numeric values\nnumeric_cols=[\"age\",\"balance\",\"day\",\"duration\",\"campaign\",\"pdays\",\"previous\"]\n\n#categories for each; some are binary(yes/no):default, housing, loan\nnon_numeric_cols=[\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\",\"contact\",\"month\",\"poutcome\"]\n\n#References\n#https://masum-math8065.medium.com/how-to-convert-categorical-data-into-numeric-in-pyspark-2202407f5fac\n#https://www.analyticsvidhya.com/blog/2019/11/build-machine-learning-pipelines-pyspark/\n\n#string to numeric\ndef convert_numeric(lst):\n    res=[]\n    for ele in lst:\n        ind=StringIndexer(inputCol=ele,outputCol=\"{0}_numeric\".format(ele))\n        res.append(ind)\n    return res\nindexed=convert_numeric(non_numeric_cols)\n\n#numeric to vector\ndef convert_vector(lst2):\n    res2=[]\n    for ele2 in lst2:\n        ind2=OneHotEncoder(inputCol=ele2.getOutputCol(), outputCol=\"{0}_encoded\".format(ele2.getOutputCol()))\n        res2.append(ind2)\n    return res2\nencoded=convert_vector(indexed)\n\n#combine all cols for getting the features\ndef featurise(lst3):\n    final=[]\n    for col in lst3:\n        final.append(col.getOutputCol())\n    return numeric_cols+final\nfeature_cols=featurise(encoded)\n\n#all numeric and encoded features into a single vector for input of model\nv_assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n\n#feature selection out of the features:https://george-jen.gitbook.io/data-science-and-apache-spark/chisqselector\nf_selector=ChiSqSelector(featuresCol=\"features\", outputCol=\"selectedFeatures\", labelCol=target_col)"],"metadata":{"id":"iey06VQfxnQg","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e07aaf5a-6fb8-425a-a3c9-f52e04e49828","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# model building (4m)\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n#Ref: https://www.silect.is/blog/random-forest-models-in-spark-ml/\nrf = RandomForestClassifier(labelCol=target_col, featuresCol=\"selectedFeatures\",numTrees=50,maxDepth=25)\npipeline = Pipeline(stages=indexed+encoded+[v_assembler,f_selector, rf]) \n\n#training data model\nmodel = pipeline.fit(bank_train)\n\n#test data \npred_test = model.transform(bank_test)\n#pred_test.show(n=1)"],"metadata":{"id":"PsIotb9ExnQh","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a04b59e4-6197-451c-8071-52526a5a724f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# model evaluation (2m)\n#like demo 3\npredictionAndLabels = pred_test.select(\"prediction\", \"label\")\nevaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",metricName=\"accuracy\")\nprint(\"Test set accuracy : \" + str(evaluator.evaluate(predictionAndLabels)))"],"metadata":{"id":"OC5ufJqAxnQh","application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"80e1c949-8291-45be-8872-c0310777c6fa","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Test set accuracy : 0.8584863412449619\n"]}],"execution_count":0}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"application/vnd.databricks.v1+notebook":{"notebookName":"cs4225_a2_databricks_student_version","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":-1,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":2202340152362715}},"nbformat":4,"nbformat_minor":0}
